{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS490 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifiers                | Score on Training | Score on Validation | Score on Leaderboard |\n",
    "|----------------------------|-------------------|---------------------|----------------------|\n",
    "| KNN (20)                   | 94.8%             | 89.8%               | 62.8%                |\n",
    "| SVM (sigmoid)              | 82.04%            | 84.6%               | 82.6%                |\n",
    "| Neural Network(15 neurons) | 72.4%             | 91.8%               | 81.2%                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code used for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTestData = np.genfromtxt(\"data/X_test.txt\", delimiter = None, skip_header=1)\n",
    "xTrainData = np.genfromtxt(\"data/X_train.txt\", delimiter = None, skip_header=1)\n",
    "yTrainData = np.genfromtxt(\"data/Y_train.txt\", delimiter = None, skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrainData, yTrainData, test_size=0.25, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainDist = yTrain[:, 1]\n",
    "yValDist = yVal[:,1]\n",
    "\n",
    "#OverSampler\n",
    "sm = SMOTE(random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the models that we trained were tested on multiple sets of features. For this model, we did reserach to find if we can use some code to figure out what the important features are for this dataset. We found that we can use the ExtraTrees classifier to see the important features. We trained that classifer with all of the features from the training data text file. This gave the results that size, current ratio, quick ratio, total margin, return on equity, net working capital, return on assets, long-term debt to equity, equity financing ratio, and debt ratio are the 10 most important. This can be seen in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "xTrainData = np.genfromtxt(\"data/X_train.txt\", delimiter = None, skip_header=1)\n",
    "yTrainData = np.genfromtxt(\"data/Y_train.txt\", delimiter = None, skip_header=1)\n",
    "X = xTrainData[:,1:35]  #independent columns\n",
    "y = yTrainData[:,1]    #target column i.e price range\n",
    "\n",
    "scale = StandardScaler().fit(X)\n",
    "xTrainScaled = scale.transform(X)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(xTrainScaled,y)\n",
    "# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data from the text file was split into 75% training data and 25% validation data. The training data was then balanced using over sampling. The training and test data were also scaled. The SVC model was trained on the 75% split. The SVC also had different options for the kernel function. Sci-kit learn gave the options to have a linear, poly, rbf and sigmoid. The one that worked the best for us was the sigmoid kernel function. To figure out which kernel function worked the best we looked at the confusion matrix and the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network was the most challenging for Chris and I since at first we thought having a lot of hidden layers and lot of neurons at each layer was how these models should be trained. We later realized that more hidden layers and neurons was on the track more towards deep learning. For this, we will talk about out best neural network performer.\n",
    "\n",
    "Our best performer was trained on one feature, the total margin. The training data was also balanced using overfitting. We did plenty of runs with all off ther different activation functions, max iterations, and the step sizes for gradient descent. We kept the default value for gradient descent since the others seemed to overfit or underfit our data each time.  We kept a max iteration of 2000, since we were getting warnings about our models not converging with a lower value.  The activation function options were identity, logistic, tanh, and relu. Logistic worked the best four the total margin feature. We chose to use one hidden layer with 15 neurons. Our scoring function ran scores for one hidden layer and tried 1-25 neurons. After multiple runs, 15 neurons seemed to appear the most with the best score and best looking confusion matrix. A higher step size for gradient descent also performed best for this model. We chose a step size of 10. Anything more or anything less for this amount of hidden layers did not perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Performer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best performer was the Support Vector classifier with the sigmoid kernel function. For our best prediction we used 10 features from the data. A Support vector Machine worked well for this since they tend to be effective for high dimensional spaces. With sci-kit learn the prediction probability function also ran the training through cross-validation making the model more accurate and precise. For Support Vector Machines, it maximizes the margin between the two classes, in this case its whether the hospital is in financial distress or not. On top of that, we trained the data with a sigmoid kernel function which would help the model train more effectively by adding curves to the decision boundries. The margins between the support vectors were not just linear. We could not graph 10 features together, but the data most likely was shaped in a way where finding the maximal margin for the decision boundry separated the data enough to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainImp = xTrain[:,[1,4,5,11,13,8,14,22,20,27]]\n",
    "xValImp = xVal[:,[1,4,5,11,13,8,14,22,20,27]]\n",
    "xTestImp = xTestData[:,[1,4,5,11,13,8,14,22,20,27]]\n",
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainImp, yTrainDist.ravel())\n",
    "\n",
    "scale = StandardScaler().fit(xTrainBal)\n",
    "xTrainScaled = scale.transform(xTrainBal)\n",
    "xTestScaled = scale.transform(xTestImp)\n",
    "\n",
    "svc = svm.SVC(kernel=\"sigmoid\", probability=True)\n",
    "svc.fit(xTrainScaled, yTrainBal)\n",
    "probs = svc.predict_proba(xTestScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good and Bad Performers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of good performers neural networks with one or two hidden layers with a logistic activation function and between 10-15 neurons at each layer performed particularly well. We did two submissions using those parameters. We trained the models with total margin as the only feature, which was balanced using overfitting. Both of these scores were 81% on the leaderboard. For this data, moving in the direction of deep learning is ineffective. More hidden layers caused overfitting since the data was not complex enough.\n",
    "\n",
    "The performer that was not so great was K-Nearest neighbors. We tried plenty of different feature combinations and various amounts of nearest neighbors. The best leaderboard score was 62%. The shape of the data is not separable enough for the nearest neighbors to train effectively on. The data could be too scattered for the feature combinations we were training on.\n",
    "\n",
    "There are a few more honorable mentions when looking at the the performance of all of the models. Our highest scores on the leaderboard had the same trend when it came to the values of the confusion matrix. When the score function returned a score of around 90% on the validation set, and the confusion matrix favored false negatives, we tended to score 75% or higher on the leaderboard. On top of that, for these same results, the f-1 score for not in distress was around 91 percent and the f-1 score for in distress was around 40%. The models that returned less false negatives did poorly on the test data. Trying to maximaize the f-1 scores also showed poor results on the test data. In all, favoring false negatives seemed to play a key role in performance on the test data. Having a higher number of false negatives proved to be a good way of balancing underfitting and overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
