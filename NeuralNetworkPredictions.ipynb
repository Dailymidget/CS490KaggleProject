{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classifier Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import NearMiss \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTestData = np.genfromtxt(\"data/X_test.txt\", delimiter = None, skip_header=1)\n",
    "xTrainData = np.genfromtxt(\"data/X_train.txt\", delimiter = None, skip_header=1)\n",
    "yTrainData = np.genfromtxt(\"data/Y_train.txt\", delimiter = None, skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrainData, yTrainData, test_size=0.25, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainDist = yTrain[:, 1]\n",
    "yTrainZ = yTrain[:, 0]\n",
    "yValDist = yVal[:,1]\n",
    "yValZ = yVal[:,0]\n",
    "\n",
    "xTestIds = xTestData[:,0]\n",
    "\n",
    "#OverSampler\n",
    "sm = SMOTE(random_state = 2)\n",
    "\n",
    "#UnderSampler\n",
    "nr = NearMiss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper method for printing Score for KNN(1-50 neighbors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printScores(xTrain, yTrain, xVal):\n",
    "    scale = StandardScaler().fit(xTrain)\n",
    "    xTrainScaled = scale.transform(xTrain)\n",
    "    xValScaled = scale.transform(xVal)\n",
    "\n",
    "    activationTypes = [\"identity\", \"logistic\", \"tanh\", \"relu\"] # Logistic is sigmoid\n",
    "    \n",
    "    for activate in activationTypes:\n",
    "        clf = MLPClassifier(activation=activate,hidden_layer_sizes=(2), random_state=1, max_iter = 2000, learning_rate_init=10)\n",
    "        clf.fit(xTrainScaled, yTrain.ravel())\n",
    "        perc = \"{:.2%}\".format(clf.score(xValScaled, yValDist))\n",
    "        yPred = clf.predict(xValScaled)\n",
    "        print(activate + \": \" + str(perc) + \"\\n\" + str(classification_report(yValDist, yPred)) + \"\\n\" + str(confusion_matrix(yValDist, yPred)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printScoresMore(xTrain, yTrain, xVal):\n",
    "    scale = StandardScaler().fit(xTrain)\n",
    "    xTrainScaled = scale.transform(xTrain)\n",
    "    xValScaled = scale.transform(xVal)\n",
    "\n",
    "    activationTypes = [\"identity\", \"logistic\", \"tanh\", \"relu\"] # Logistic is sigmoid\n",
    "    \n",
    "    for activate in activationTypes:\n",
    "        for i in range(2,25, 1):\n",
    "            clf = MLPClassifier(activation=activate,hidden_layer_sizes=(i), random_state=1, max_iter = 2000, learning_rate_init=11)\n",
    "            clf.fit(xTrainScaled, yTrain.ravel())\n",
    "            perc = \"{:.2%}\".format(clf.score(xValScaled, yValDist))\n",
    "            yPred = clf.predict(xValScaled)\n",
    "            tn, fp, fn, tp = confusion_matrix(yValDist, yPred).ravel()\n",
    "            if (fn > 20 and tp > 20 and tn > 20 and fp <= 100):\n",
    "                print(activate + \" and \" + str(i) + \"neurons: \" + str(perc) + \"\\n\" + str(classification_report(yValDist, yPred)) + \"\\n\" + str(confusion_matrix(yValDist, yPred)) + \"\\n\")\n",
    "                print(\"tn: \" + str(tn))\n",
    "                print(\"fp: \" + str(fp))\n",
    "                print(\"fn: \" + str(fn))\n",
    "                print(\"tp: \" + str(tp))\n",
    "                print(\"\\n\")\n",
    "                perc = \"{:.2%}\".format(clf.score(xTrainScaled, yTrain))\n",
    "                print(perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrainImp = xTrain[:,[1,4,5,11,13,8,14]]\n",
    "# xValImp = xVal[:,[1,4,5,11,13,8,14]]\n",
    "xTrainImp = xTrain[:,[11]]\n",
    "xValImp = xVal[:,[11]]\n",
    "#xTestImp = xTestData[:,[1,4,5,11,13,8,14]]\n",
    "xTestImp = xTestData[:,[11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity and 2neurons: 93.13%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.96      1136\n",
      "         1.0       0.36      0.55      0.44        58\n",
      "\n",
      "    accuracy                           0.93      1194\n",
      "   macro avg       0.67      0.75      0.70      1194\n",
      "weighted avg       0.95      0.93      0.94      1194\n",
      "\n",
      "[[1080   56]\n",
      " [  26   32]]\n",
      "\n",
      "tn: 1080\n",
      "fp: 56\n",
      "fn: 26\n",
      "tp: 32\n",
      "\n",
      "\n",
      "72.53%\n",
      "identity and 9neurons: 90.79%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.92      0.95      1136\n",
      "         1.0       0.29      0.64      0.40        58\n",
      "\n",
      "    accuracy                           0.91      1194\n",
      "   macro avg       0.64      0.78      0.68      1194\n",
      "weighted avg       0.95      0.91      0.92      1194\n",
      "\n",
      "[[1047   89]\n",
      " [  21   37]]\n",
      "\n",
      "tn: 1047\n",
      "fp: 89\n",
      "fn: 21\n",
      "tp: 37\n",
      "\n",
      "\n",
      "72.18%\n",
      "identity and 15neurons: 94.64%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97      1136\n",
      "         1.0       0.45      0.45      0.45        58\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.71      0.71      0.71      1194\n",
      "weighted avg       0.95      0.95      0.95      1194\n",
      "\n",
      "[[1104   32]\n",
      " [  32   26]]\n",
      "\n",
      "tn: 1104\n",
      "fp: 32\n",
      "fn: 32\n",
      "tp: 26\n",
      "\n",
      "\n",
      "66.27%\n",
      "identity and 20neurons: 90.70%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.92      0.95      1136\n",
      "         1.0       0.29      0.64      0.40        58\n",
      "\n",
      "    accuracy                           0.91      1194\n",
      "   macro avg       0.64      0.78      0.67      1194\n",
      "weighted avg       0.95      0.91      0.92      1194\n",
      "\n",
      "[[1046   90]\n",
      " [  21   37]]\n",
      "\n",
      "tn: 1046\n",
      "fp: 90\n",
      "fn: 21\n",
      "tp: 37\n",
      "\n",
      "\n",
      "72.39%\n",
      "identity and 21neurons: 94.64%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97      1136\n",
      "         1.0       0.45      0.45      0.45        58\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.71      0.71      0.71      1194\n",
      "weighted avg       0.95      0.95      0.95      1194\n",
      "\n",
      "[[1104   32]\n",
      " [  32   26]]\n",
      "\n",
      "tn: 1104\n",
      "fp: 32\n",
      "fn: 32\n",
      "tp: 26\n",
      "\n",
      "\n",
      "66.22%\n",
      "logistic and 10neurons: 93.13%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.96      1136\n",
      "         1.0       0.37      0.57      0.45        58\n",
      "\n",
      "    accuracy                           0.93      1194\n",
      "   macro avg       0.67      0.76      0.70      1194\n",
      "weighted avg       0.95      0.93      0.94      1194\n",
      "\n",
      "[[1079   57]\n",
      " [  25   33]]\n",
      "\n",
      "tn: 1079\n",
      "fp: 57\n",
      "fn: 25\n",
      "tp: 33\n",
      "\n",
      "\n",
      "72.65%\n",
      "logistic and 11neurons: 91.79%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.93      0.96      1136\n",
      "         1.0       0.32      0.62      0.42        58\n",
      "\n",
      "    accuracy                           0.92      1194\n",
      "   macro avg       0.65      0.78      0.69      1194\n",
      "weighted avg       0.95      0.92      0.93      1194\n",
      "\n",
      "[[1060   76]\n",
      " [  22   36]]\n",
      "\n",
      "tn: 1060\n",
      "fp: 76\n",
      "fn: 22\n",
      "tp: 36\n",
      "\n",
      "\n",
      "72.37%\n",
      "logistic and 21neurons: 93.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.97      1136\n",
      "         1.0       0.38      0.52      0.44        58\n",
      "\n",
      "    accuracy                           0.94      1194\n",
      "   macro avg       0.68      0.74      0.70      1194\n",
      "weighted avg       0.95      0.94      0.94      1194\n",
      "\n",
      "[[1088   48]\n",
      " [  28   30]]\n",
      "\n",
      "tn: 1088\n",
      "fp: 48\n",
      "fn: 28\n",
      "tp: 30\n",
      "\n",
      "\n",
      "71.93%\n",
      "logistic and 23neurons: 92.80%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.96      1136\n",
      "         1.0       0.35      0.55      0.43        58\n",
      "\n",
      "    accuracy                           0.93      1194\n",
      "   macro avg       0.66      0.75      0.69      1194\n",
      "weighted avg       0.95      0.93      0.94      1194\n",
      "\n",
      "[[1076   60]\n",
      " [  26   32]]\n",
      "\n",
      "tn: 1076\n",
      "fp: 60\n",
      "fn: 26\n",
      "tp: 32\n",
      "\n",
      "\n",
      "72.24%\n",
      "tanh and 5neurons: 92.38%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.94      0.96      1136\n",
      "         1.0       0.34      0.62      0.44        58\n",
      "\n",
      "    accuracy                           0.92      1194\n",
      "   macro avg       0.66      0.78      0.70      1194\n",
      "weighted avg       0.95      0.92      0.93      1194\n",
      "\n",
      "[[1067   69]\n",
      " [  22   36]]\n",
      "\n",
      "tn: 1067\n",
      "fp: 69\n",
      "fn: 22\n",
      "tp: 36\n",
      "\n",
      "\n",
      "72.48%\n",
      "tanh and 10neurons: 93.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.96      1136\n",
      "         1.0       0.36      0.55      0.44        58\n",
      "\n",
      "    accuracy                           0.93      1194\n",
      "   macro avg       0.67      0.75      0.70      1194\n",
      "weighted avg       0.95      0.93      0.94      1194\n",
      "\n",
      "[[1079   57]\n",
      " [  26   32]]\n",
      "\n",
      "tn: 1079\n",
      "fp: 57\n",
      "fn: 26\n",
      "tp: 32\n",
      "\n",
      "\n",
      "72.62%\n",
      "tanh and 14neurons: 94.14%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.97      1136\n",
      "         1.0       0.41      0.50      0.45        58\n",
      "\n",
      "    accuracy                           0.94      1194\n",
      "   macro avg       0.69      0.73      0.71      1194\n",
      "weighted avg       0.95      0.94      0.94      1194\n",
      "\n",
      "[[1095   41]\n",
      " [  29   29]]\n",
      "\n",
      "tn: 1095\n",
      "fp: 41\n",
      "fn: 29\n",
      "tp: 29\n",
      "\n",
      "\n",
      "69.33%\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainImp, yTrainDist.ravel())\n",
    "\n",
    "printScoresMore(xTrainBal, yTrainBal, xValImp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Features for Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainFeat = xTrain[:,[1,4,5,11,8,14,22]]\n",
    "xValFeat = xVal[:,[1,4,5,11,8,14,22]]\n",
    "xTestFeat = xTestData[:,[1,4,5,11,8,14,22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 91.37%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.92      0.95      1138\n",
      "         1.0       0.33      0.80      0.47        56\n",
      "\n",
      "    accuracy                           0.91      1194\n",
      "   macro avg       0.66      0.86      0.71      1194\n",
      "weighted avg       0.96      0.91      0.93      1194\n",
      "\n",
      "[[1046   92]\n",
      " [  11   45]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 98.49%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      1138\n",
      "         1.0       0.83      0.86      0.84        56\n",
      "\n",
      "    accuracy                           0.98      1194\n",
      "   macro avg       0.91      0.92      0.92      1194\n",
      "weighted avg       0.99      0.98      0.99      1194\n",
      "\n",
      "[[1128   10]\n",
      " [   8   48]]\n",
      "\n",
      "relu: 98.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99      1138\n",
      "         1.0       0.84      0.93      0.88        56\n",
      "\n",
      "    accuracy                           0.99      1194\n",
      "   macro avg       0.92      0.96      0.94      1194\n",
      "weighted avg       0.99      0.99      0.99      1194\n",
      "\n",
      "[[1128   10]\n",
      " [   4   52]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainFeat, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValFeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainAll = xTrain[:, 1:35]\n",
    "xValAll = xVal[:, 1:35]\n",
    "xTestAll = xTestData[:, 1:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 97.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99      1138\n",
      "         1.0       0.79      0.54      0.64        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.88      0.76      0.81      1194\n",
      "weighted avg       0.97      0.97      0.97      1194\n",
      "\n",
      "[[1130    8]\n",
      " [  26   30]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.48%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.68      0.48      0.56        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.82      0.74      0.77      1194\n",
      "weighted avg       0.96      0.96      0.96      1194\n",
      "\n",
      "[[1125   13]\n",
      " [  29   27]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainAll, yTrainDist, xValAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 97.82%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      1138\n",
      "         1.0       0.74      0.82      0.78        56\n",
      "\n",
      "    accuracy                           0.98      1194\n",
      "   macro avg       0.87      0.90      0.88      1194\n",
      "weighted avg       0.98      0.98      0.98      1194\n",
      "\n",
      "[[1122   16]\n",
      " [  10   46]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98      1138\n",
      "         1.0       0.63      0.71      0.67        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.81      0.85      0.83      1194\n",
      "weighted avg       0.97      0.97      0.97      1194\n",
      "\n",
      "[[1115   23]\n",
      " [  16   40]]\n",
      "\n",
      "relu: 98.16%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      1138\n",
      "         1.0       0.81      0.79      0.80        56\n",
      "\n",
      "    accuracy                           0.98      1194\n",
      "   macro avg       0.90      0.89      0.90      1194\n",
      "weighted avg       0.98      0.98      0.98      1194\n",
      "\n",
      "[[1128   10]\n",
      " [  12   44]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainAll, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 91.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.91      0.95      1138\n",
      "         1.0       0.33      0.89      0.49        56\n",
      "\n",
      "    accuracy                           0.91      1194\n",
      "   macro avg       0.66      0.90      0.72      1194\n",
      "weighted avg       0.96      0.91      0.93      1194\n",
      "\n",
      "[[1038  100]\n",
      " [   6   50]]\n",
      "\n",
      "logistic: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh: 94.89%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97      1138\n",
      "         1.0       0.48      0.91      0.63        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.74      0.93      0.80      1194\n",
      "weighted avg       0.97      0.95      0.96      1194\n",
      "\n",
      "[[1082   56]\n",
      " [   5   51]]\n",
      "\n",
      "relu: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainAll, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All first Features of each Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainFirsts = xTrain[:,[1,2,3, 4, 10, 20, 23, 27,30]]\n",
    "xValFirsts = xVal[:,[1,2,3, 4, 10, 20, 23, 27,30]]\n",
    "xTestFirsts = xTestData[:,[1,2,3, 4, 10, 20, 23, 27,30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 97.32%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99      1138\n",
      "         1.0       0.80      0.57      0.67        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.89      0.78      0.83      1194\n",
      "weighted avg       0.97      0.97      0.97      1194\n",
      "\n",
      "[[1130    8]\n",
      " [  24   32]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 97.91%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99      1138\n",
      "         1.0       0.73      0.88      0.80        56\n",
      "\n",
      "    accuracy                           0.98      1194\n",
      "   macro avg       0.86      0.93      0.89      1194\n",
      "weighted avg       0.98      0.98      0.98      1194\n",
      "\n",
      "[[1120   18]\n",
      " [   7   49]]\n",
      "\n",
      "relu: 97.91%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      1138\n",
      "         1.0       0.79      0.75      0.77        56\n",
      "\n",
      "    accuracy                           0.98      1194\n",
      "   macro avg       0.89      0.87      0.88      1194\n",
      "weighted avg       0.98      0.98      0.98      1194\n",
      "\n",
      "[[1127   11]\n",
      " [  14   42]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainFirsts, yTrainDist, xValFirsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 95.14%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97      1138\n",
      "         1.0       0.49      0.96      0.65        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.74      0.96      0.81      1194\n",
      "weighted avg       0.97      0.95      0.96      1194\n",
      "\n",
      "[[1082   56]\n",
      " [   2   54]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.57%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.98      1138\n",
      "         1.0       0.59      0.91      0.71        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.79      0.94      0.85      1194\n",
      "weighted avg       0.98      0.97      0.97      1194\n",
      "\n",
      "[[1102   36]\n",
      " [   5   51]]\n",
      "\n",
      "relu: 97.32%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99      1138\n",
      "         1.0       0.67      0.84      0.75        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.83      0.91      0.87      1194\n",
      "weighted avg       0.98      0.97      0.97      1194\n",
      "\n",
      "[[1115   23]\n",
      " [   9   47]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainFirsts, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValFirsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 94.47%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.95      0.97      1138\n",
      "         1.0       0.45      0.80      0.58        56\n",
      "\n",
      "    accuracy                           0.94      1194\n",
      "   macro avg       0.72      0.88      0.77      1194\n",
      "weighted avg       0.96      0.94      0.95      1194\n",
      "\n",
      "[[1083   55]\n",
      " [  11   45]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 94.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.94      0.97      1138\n",
      "         1.0       0.43      0.89      0.58        56\n",
      "\n",
      "    accuracy                           0.94      1194\n",
      "   macro avg       0.71      0.92      0.78      1194\n",
      "weighted avg       0.97      0.94      0.95      1194\n",
      "\n",
      "[[1073   65]\n",
      " [   6   50]]\n",
      "\n",
      "relu: 94.14%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.94      0.97      1138\n",
      "         1.0       0.44      0.89      0.59        56\n",
      "\n",
      "    accuracy                           0.94      1194\n",
      "   macro avg       0.72      0.92      0.78      1194\n",
      "weighted avg       0.97      0.94      0.95      1194\n",
      "\n",
      "[[1074   64]\n",
      " [   6   50]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainFirsts, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValFirsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All second Features of each Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainSeconds = xTrain[:,[1,2,3, 5, 11, 21, 24, 28,31]]\n",
    "xValSeconds = xVal[:,[1,2,3, 5, 11, 21, 24, 28,31]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 96.40%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.74      0.36      0.48        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.85      0.68      0.73      1194\n",
      "weighted avg       0.96      0.96      0.96      1194\n",
      "\n",
      "[[1131    7]\n",
      " [  36   20]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98      1138\n",
      "         1.0       0.67      0.70      0.68        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.83      0.84      0.83      1194\n",
      "weighted avg       0.97      0.97      0.97      1194\n",
      "\n",
      "[[1119   19]\n",
      " [  17   39]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainSeconds, yTrainDist, xValSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 90.45%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.91      0.95      1138\n",
      "         1.0       0.30      0.80      0.44        56\n",
      "\n",
      "    accuracy                           0.90      1194\n",
      "   macro avg       0.65      0.86      0.69      1194\n",
      "weighted avg       0.96      0.90      0.92      1194\n",
      "\n",
      "[[1035  103]\n",
      " [  11   45]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 94.89%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.95      0.97      1138\n",
      "         1.0       0.48      0.88      0.62        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.73      0.91      0.79      1194\n",
      "weighted avg       0.97      0.95      0.96      1194\n",
      "\n",
      "[[1084   54]\n",
      " [   7   49]]\n",
      "\n",
      "relu: 96.48%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98      1138\n",
      "         1.0       0.59      0.82      0.69        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.79      0.90      0.83      1194\n",
      "weighted avg       0.97      0.96      0.97      1194\n",
      "\n",
      "[[1106   32]\n",
      " [  10   46]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainSeconds, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 86.35%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.86      0.92      1138\n",
      "         1.0       0.23      0.84      0.37        56\n",
      "\n",
      "    accuracy                           0.86      1194\n",
      "   macro avg       0.61      0.85      0.64      1194\n",
      "weighted avg       0.96      0.86      0.90      1194\n",
      "\n",
      "[[984 154]\n",
      " [  9  47]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 74.20%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.74      0.85      1138\n",
      "         1.0       0.13      0.82      0.23        56\n",
      "\n",
      "    accuracy                           0.74      1194\n",
      "   macro avg       0.56      0.78      0.54      1194\n",
      "weighted avg       0.95      0.74      0.82      1194\n",
      "\n",
      "[[840 298]\n",
      " [ 10  46]]\n",
      "\n",
      "relu: 61.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.60      0.75      1138\n",
      "         1.0       0.09      0.84      0.17        56\n",
      "\n",
      "    accuracy                           0.61      1194\n",
      "   macro avg       0.54      0.72      0.46      1194\n",
      "weighted avg       0.95      0.61      0.72      1194\n",
      "\n",
      "[[682 456]\n",
      " [  9  47]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainSeconds, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All third features of each Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainThirds = xTrain[:,[1,2,3, 6, 12, 22, 25, 29,32]]\n",
    "xValThirds = xVal[:,[1,2,3, 6, 12, 22, 25, 29,32]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 95.23%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98      1138\n",
      "         1.0       0.46      0.11      0.17        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.71      0.55      0.57      1194\n",
      "weighted avg       0.93      0.95      0.94      1194\n",
      "\n",
      "[[1131    7]\n",
      " [  50    6]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98      1138\n",
      "         1.0       0.59      0.54      0.56        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.78      0.76      0.77      1194\n",
      "weighted avg       0.96      0.96      0.96      1194\n",
      "\n",
      "[[1117   21]\n",
      " [  26   30]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainThirds, yTrainDist, xValThirds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 82.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.90      1138\n",
      "         1.0       0.17      0.68      0.27        56\n",
      "\n",
      "    accuracy                           0.83      1194\n",
      "   macro avg       0.58      0.76      0.59      1194\n",
      "weighted avg       0.94      0.83      0.87      1194\n",
      "\n",
      "[[951 187]\n",
      " [ 18  38]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 95.14%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.96      0.97      1138\n",
      "         1.0       0.49      0.79      0.60        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.74      0.87      0.79      1194\n",
      "weighted avg       0.97      0.95      0.96      1194\n",
      "\n",
      "[[1092   46]\n",
      " [  12   44]]\n",
      "\n",
      "relu: 92.46%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.94      0.96      1138\n",
      "         1.0       0.35      0.70      0.46        56\n",
      "\n",
      "    accuracy                           0.92      1194\n",
      "   macro avg       0.67      0.82      0.71      1194\n",
      "weighted avg       0.95      0.92      0.94      1194\n",
      "\n",
      "[[1065   73]\n",
      " [  17   39]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainThirds, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValThirds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 83.42%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91      1138\n",
      "         1.0       0.18      0.71      0.29        56\n",
      "\n",
      "    accuracy                           0.83      1194\n",
      "   macro avg       0.58      0.78      0.60      1194\n",
      "weighted avg       0.95      0.83      0.88      1194\n",
      "\n",
      "[[956 182]\n",
      " [ 16  40]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 81.16%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.81      0.89      1138\n",
      "         1.0       0.18      0.82      0.29        56\n",
      "\n",
      "    accuracy                           0.81      1194\n",
      "   macro avg       0.58      0.82      0.59      1194\n",
      "weighted avg       0.95      0.81      0.86      1194\n",
      "\n",
      "[[923 215]\n",
      " [ 10  46]]\n",
      "\n",
      "relu: 76.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.76      0.86      1138\n",
      "         1.0       0.13      0.75      0.23        56\n",
      "\n",
      "    accuracy                           0.76      1194\n",
      "   macro avg       0.56      0.76      0.54      1194\n",
      "weighted avg       0.94      0.76      0.83      1194\n",
      "\n",
      "[[866 272]\n",
      " [ 14  42]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainThirds, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValThirds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls and Liquidity Ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainLR = xTrain[:,[1,2,3, 4, 5, 6, 7, 8,9]]\n",
    "xValLR = xVal[:,[1,2,3, 4, 5, 6, 7, 8,9]]\n",
    "xTestLR = xTrainData[:,[1,2,3, 4, 5, 6, 7, 8,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 95.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.67      0.29      0.40        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.82      0.64      0.69      1194\n",
      "weighted avg       0.95      0.96      0.95      1194\n",
      "\n",
      "[[1130    8]\n",
      " [  40   16]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.57%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.71      0.45      0.55        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.84      0.72      0.77      1194\n",
      "weighted avg       0.96      0.97      0.96      1194\n",
      "\n",
      "[[1128   10]\n",
      " [  31   25]]\n",
      "\n",
      "relu: 96.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98      1138\n",
      "         1.0       0.61      0.61      0.61        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.79      0.79      0.79      1194\n",
      "weighted avg       0.96      0.96      0.96      1194\n",
      "\n",
      "[[1116   22]\n",
      " [  22   34]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainLR, yTrainDist, xValLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 86.68%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.87      0.93      1138\n",
      "         1.0       0.23      0.80      0.36        56\n",
      "\n",
      "    accuracy                           0.87      1194\n",
      "   macro avg       0.61      0.84      0.64      1194\n",
      "weighted avg       0.95      0.87      0.90      1194\n",
      "\n",
      "[[990 148]\n",
      " [ 11  45]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 90.70%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.91      0.95      1138\n",
      "         1.0       0.30      0.77      0.44        56\n",
      "\n",
      "    accuracy                           0.91      1194\n",
      "   macro avg       0.65      0.84      0.69      1194\n",
      "weighted avg       0.96      0.91      0.93      1194\n",
      "\n",
      "[[1040   98]\n",
      " [  13   43]]\n",
      "\n",
      "relu: 93.89%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.95      0.97      1138\n",
      "         1.0       0.41      0.71      0.52        56\n",
      "\n",
      "    accuracy                           0.94      1194\n",
      "   macro avg       0.70      0.83      0.75      1194\n",
      "weighted avg       0.96      0.94      0.95      1194\n",
      "\n",
      "[[1081   57]\n",
      " [  16   40]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainLR, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 78.64%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.79      0.88      1138\n",
      "         1.0       0.14      0.68      0.23        56\n",
      "\n",
      "    accuracy                           0.79      1194\n",
      "   macro avg       0.56      0.74      0.55      1194\n",
      "weighted avg       0.94      0.79      0.85      1194\n",
      "\n",
      "[[901 237]\n",
      " [ 18  38]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 43.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.42      0.59      1138\n",
      "         1.0       0.06      0.77      0.11        56\n",
      "\n",
      "    accuracy                           0.44      1194\n",
      "   macro avg       0.52      0.59      0.35      1194\n",
      "weighted avg       0.93      0.44      0.57      1194\n",
      "\n",
      "[[479 659]\n",
      " [ 13  43]]\n",
      "\n",
      "relu: 44.14%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.43      0.59      1138\n",
      "         1.0       0.06      0.73      0.11        56\n",
      "\n",
      "    accuracy                           0.44      1194\n",
      "   macro avg       0.51      0.58      0.35      1194\n",
      "weighted avg       0.93      0.44      0.57      1194\n",
      "\n",
      "[[486 652]\n",
      " [ 15  41]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainLR, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls and Profitability Ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainPR = xTrain[:,[1, 2, 3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "xValPR = xVal[:,[1, 2, 3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 95.48%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98      1138\n",
      "         1.0       0.58      0.12      0.21        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.77      0.56      0.59      1194\n",
      "weighted avg       0.94      0.95      0.94      1194\n",
      "\n",
      "[[1133    5]\n",
      " [  49    7]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 95.56%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98      1138\n",
      "         1.0       0.57      0.21      0.31        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.77      0.60      0.64      1194\n",
      "weighted avg       0.94      0.96      0.95      1194\n",
      "\n",
      "[[1129    9]\n",
      " [  44   12]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainPR, yTrainDist, xValPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 85.76%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.87      0.92      1138\n",
      "         1.0       0.19      0.62      0.29        56\n",
      "\n",
      "    accuracy                           0.86      1194\n",
      "   macro avg       0.58      0.75      0.61      1194\n",
      "weighted avg       0.94      0.86      0.89      1194\n",
      "\n",
      "[[989 149]\n",
      " [ 21  35]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 90.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.91      0.95      1138\n",
      "         1.0       0.28      0.70      0.40        56\n",
      "\n",
      "    accuracy                           0.90      1194\n",
      "   macro avg       0.63      0.80      0.67      1194\n",
      "weighted avg       0.95      0.90      0.92      1194\n",
      "\n",
      "[[1037  101]\n",
      " [  17   39]]\n",
      "\n",
      "relu: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainPR, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 84.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92      1138\n",
      "         1.0       0.18      0.61      0.27        56\n",
      "\n",
      "    accuracy                           0.85      1194\n",
      "   macro avg       0.58      0.73      0.60      1194\n",
      "weighted avg       0.94      0.85      0.89      1194\n",
      "\n",
      "[[980 158]\n",
      " [ 22  34]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n",
      "tanh: 40.37%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.39      0.55      1138\n",
      "         1.0       0.06      0.75      0.11        56\n",
      "\n",
      "    accuracy                           0.40      1194\n",
      "   macro avg       0.51      0.57      0.33      1194\n",
      "weighted avg       0.93      0.40      0.53      1194\n",
      "\n",
      "[[440 698]\n",
      " [ 14  42]]\n",
      "\n",
      "relu: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainPR, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls and Profitability Ratios Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainPRB = xTrain[:,[1, 2, 3, 13, 14, 15, 16, 19]]\n",
    "xValPRB = xVal[:,[1, 2, 3, 13, 14, 15, 16, 19]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 95.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98      1138\n",
      "         1.0       0.69      0.16      0.26        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.83      0.58      0.62      1194\n",
      "weighted avg       0.95      0.96      0.94      1194\n",
      "\n",
      "[[1134    4]\n",
      " [  47    9]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainPRB, yTrainDist, xValPRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 87.19%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.88      0.93      1138\n",
      "         1.0       0.21      0.61      0.31        56\n",
      "\n",
      "    accuracy                           0.87      1194\n",
      "   macro avg       0.59      0.75      0.62      1194\n",
      "weighted avg       0.94      0.87      0.90      1194\n",
      "\n",
      "[[1007  131]\n",
      " [  22   34]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n",
      "tanh: 86.43%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.88      0.92      1138\n",
      "         1.0       0.20      0.62      0.30        56\n",
      "\n",
      "    accuracy                           0.86      1194\n",
      "   macro avg       0.59      0.75      0.61      1194\n",
      "weighted avg       0.94      0.86      0.90      1194\n",
      "\n",
      "[[997 141]\n",
      " [ 21  35]]\n",
      "\n",
      "relu: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainPRB, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValPRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 62.23%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.63      0.76      1138\n",
      "         1.0       0.06      0.50      0.11        56\n",
      "\n",
      "    accuracy                           0.62      1194\n",
      "   macro avg       0.51      0.56      0.44      1194\n",
      "weighted avg       0.92      0.62      0.73      1194\n",
      "\n",
      "[[715 423]\n",
      " [ 28  28]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n",
      "tanh: 82.66%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90      1138\n",
      "         1.0       0.10      0.32      0.15        56\n",
      "\n",
      "    accuracy                           0.83      1194\n",
      "   macro avg       0.53      0.59      0.53      1194\n",
      "weighted avg       0.92      0.83      0.87      1194\n",
      "\n",
      "[[969 169]\n",
      " [ 38  18]]\n",
      "\n",
      "relu: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainPRB, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValPRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls and Profitability Ratios Non-Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainPRNB = xTrain[:,[1, 2, 3, 10, 11, 12, 17, 18]]\n",
    "xValPRNB = xVal[:,[1, 2, 3, 10, 11, 12, 17, 18]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 95.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98      1138\n",
      "         1.0       0.69      0.16      0.26        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.83      0.58      0.62      1194\n",
      "weighted avg       0.95      0.96      0.94      1194\n",
      "\n",
      "[[1134    4]\n",
      " [  47    9]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainPRB, yTrainDist, xValPRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 84.25%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91      1138\n",
      "         1.0       0.18      0.64      0.28        56\n",
      "\n",
      "    accuracy                           0.84      1194\n",
      "   macro avg       0.58      0.75      0.59      1194\n",
      "weighted avg       0.94      0.84      0.88      1194\n",
      "\n",
      "[[970 168]\n",
      " [ 20  36]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n",
      "tanh: 86.85%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.88      0.93      1138\n",
      "         1.0       0.21      0.64      0.31        56\n",
      "\n",
      "    accuracy                           0.87      1194\n",
      "   macro avg       0.59      0.76      0.62      1194\n",
      "weighted avg       0.94      0.87      0.90      1194\n",
      "\n",
      "[[1001  137]\n",
      " [  20   36]]\n",
      "\n",
      "relu: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainPRNB, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValPRNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 80.32%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.81      0.89      1138\n",
      "         1.0       0.15      0.68      0.24        56\n",
      "\n",
      "    accuracy                           0.80      1194\n",
      "   macro avg       0.56      0.74      0.57      1194\n",
      "weighted avg       0.94      0.80      0.86      1194\n",
      "\n",
      "[[921 217]\n",
      " [ 18  38]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n",
      "tanh: 84.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.86      0.91      1138\n",
      "         1.0       0.15      0.52      0.23        56\n",
      "\n",
      "    accuracy                           0.84      1194\n",
      "   macro avg       0.56      0.69      0.57      1194\n",
      "weighted avg       0.93      0.84      0.88      1194\n",
      "\n",
      "[[974 164]\n",
      " [ 27  29]]\n",
      "\n",
      "relu: 41.29%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.40      0.56      1138\n",
      "         1.0       0.06      0.77      0.11        56\n",
      "\n",
      "    accuracy                           0.41      1194\n",
      "   macro avg       0.52      0.58      0.34      1194\n",
      "weighted avg       0.93      0.41      0.54      1194\n",
      "\n",
      "[[450 688]\n",
      " [ 13  43]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainPRNB, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValPRNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Capital Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainCS = xTrain[:,[1, 2, 3, 20, 21, 22]]\n",
    "xValCS = xVal[:,[1, 2, 3, 20, 21, 22]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 96.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.63      0.43      0.51        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.80      0.71      0.75      1194\n",
      "weighted avg       0.96      0.96      0.96      1194\n",
      "\n",
      "[[1124   14]\n",
      " [  32   24]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.79      0.48      0.60        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.88      0.74      0.79      1194\n",
      "weighted avg       0.97      0.97      0.97      1194\n",
      "\n",
      "[[1131    7]\n",
      " [  29   27]]\n",
      "\n",
      "relu: 96.90%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98      1138\n",
      "         1.0       0.65      0.73      0.69        56\n",
      "\n",
      "    accuracy                           0.97      1194\n",
      "   macro avg       0.82      0.86      0.84      1194\n",
      "weighted avg       0.97      0.97      0.97      1194\n",
      "\n",
      "[[1116   22]\n",
      " [  15   41]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainCS, yTrainDist, xValCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 91.46%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.92      0.95      1138\n",
      "         1.0       0.34      0.86      0.48        56\n",
      "\n",
      "    accuracy                           0.91      1194\n",
      "   macro avg       0.67      0.89      0.72      1194\n",
      "weighted avg       0.96      0.91      0.93      1194\n",
      "\n",
      "[[1044   94]\n",
      " [   8   48]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 69.26%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.68      0.81      1138\n",
      "         1.0       0.12      0.91      0.22        56\n",
      "\n",
      "    accuracy                           0.69      1194\n",
      "   macro avg       0.56      0.80      0.51      1194\n",
      "weighted avg       0.95      0.69      0.78      1194\n",
      "\n",
      "[[776 362]\n",
      " [  5  51]]\n",
      "\n",
      "relu: 67.84%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.67      0.80      1138\n",
      "         1.0       0.12      0.91      0.21        56\n",
      "\n",
      "    accuracy                           0.68      1194\n",
      "   macro avg       0.56      0.79      0.50      1194\n",
      "weighted avg       0.95      0.68      0.77      1194\n",
      "\n",
      "[[759 379]\n",
      " [  5  51]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainCS, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 92.80%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96      1138\n",
      "         1.0       0.37      0.79      0.51        56\n",
      "\n",
      "    accuracy                           0.93      1194\n",
      "   macro avg       0.68      0.86      0.73      1194\n",
      "weighted avg       0.96      0.93      0.94      1194\n",
      "\n",
      "[[1064   74]\n",
      " [  12   44]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 79.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88      1138\n",
      "         1.0       0.17      0.86      0.28        56\n",
      "\n",
      "    accuracy                           0.79      1194\n",
      "   macro avg       0.58      0.82      0.58      1194\n",
      "weighted avg       0.95      0.79      0.85      1194\n",
      "\n",
      "[[899 239]\n",
      " [  8  48]]\n",
      "\n",
      "relu: 50.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.48      0.65      1138\n",
      "         1.0       0.08      0.88      0.14        56\n",
      "\n",
      "    accuracy                           0.50      1194\n",
      "   macro avg       0.53      0.68      0.39      1194\n",
      "weighted avg       0.94      0.50      0.62      1194\n",
      "\n",
      "[[548 590]\n",
      " [  7  49]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainCS, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Tests 4/20/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainNT = xTrain[:,[1,2,3, 4, 5, 6, 7, 8,9,26]]\n",
    "xValNT = xVal[:,[1,2,3, 4, 5, 6, 7, 8,9,26]]\n",
    "xTestNT = xTestData[:,[1,2,3, 4, 5, 6, 7, 8,9,26]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 95.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.67      0.29      0.40        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.82      0.64      0.69      1194\n",
      "weighted avg       0.95      0.96      0.95      1194\n",
      "\n",
      "[[1130    8]\n",
      " [  40   16]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 96.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1138\n",
      "         1.0       0.66      0.38      0.48        56\n",
      "\n",
      "    accuracy                           0.96      1194\n",
      "   macro avg       0.81      0.68      0.73      1194\n",
      "weighted avg       0.96      0.96      0.96      1194\n",
      "\n",
      "[[1127   11]\n",
      " [  35   21]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printScores(xTrainNT, yTrainDist, xValNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 85.76%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.86      0.92      1138\n",
      "         1.0       0.22      0.82      0.35        56\n",
      "\n",
      "    accuracy                           0.86      1194\n",
      "   macro avg       0.61      0.84      0.64      1194\n",
      "weighted avg       0.95      0.86      0.89      1194\n",
      "\n",
      "[[978 160]\n",
      " [ 10  46]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n",
      "tanh: 93.80%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.95      0.97      1138\n",
      "         1.0       0.41      0.75      0.53        56\n",
      "\n",
      "    accuracy                           0.94      1194\n",
      "   macro avg       0.70      0.85      0.75      1194\n",
      "weighted avg       0.96      0.94      0.95      1194\n",
      "\n",
      "[[1078   60]\n",
      " [  14   42]]\n",
      "\n",
      "relu: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainNT, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: 85.51%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.87      0.92      1138\n",
      "         1.0       0.19      0.64      0.29        56\n",
      "\n",
      "    accuracy                           0.86      1194\n",
      "   macro avg       0.59      0.75      0.61      1194\n",
      "weighted avg       0.94      0.86      0.89      1194\n",
      "\n",
      "[[985 153]\n",
      " [ 20  36]]\n",
      "\n",
      "logistic: 95.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98      1138\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.95      1194\n",
      "   macro avg       0.48      0.50      0.49      1194\n",
      "weighted avg       0.91      0.95      0.93      1194\n",
      "\n",
      "[[1138    0]\n",
      " [  56    0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh: 85.43%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92      1138\n",
      "         1.0       0.19      0.66      0.30        56\n",
      "\n",
      "    accuracy                           0.85      1194\n",
      "   macro avg       0.59      0.76      0.61      1194\n",
      "weighted avg       0.94      0.85      0.89      1194\n",
      "\n",
      "[[983 155]\n",
      " [ 19  37]]\n",
      "\n",
      "relu: 4.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      1138\n",
      "         1.0       0.05      1.00      0.09        56\n",
      "\n",
      "    accuracy                           0.05      1194\n",
      "   macro avg       0.02      0.50      0.04      1194\n",
      "weighted avg       0.00      0.05      0.00      1194\n",
      "\n",
      "[[   0 1138]\n",
      " [   0   56]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = nr.fit_sample(xTrainNT, yTrainDist.ravel())\n",
    "\n",
    "printScores(xTrainBal, yTrainBal, xValNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best UnderSampling Performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrainBal, yTrainBal = nr.fit_sample(xTrainNT, yTrainDist.ravel())\n",
    "\n",
    "# scale = StandardScaler().fit(xTrainBal)\n",
    "# xTrainScaled = scale.transform(xTrainBal)\n",
    "# xTestScaled = scale.transform(xTestNT)\n",
    "\n",
    "# svc = svm.SVC(kernel=\"poly\", probability=True)\n",
    "# svc.fit(xTrainScaled, yTrainBal)\n",
    "# probs = svc.predict_proba(xTestScaled)\n",
    "# ids = xTestIds.tolist()\n",
    "# probs = probs.tolist()\n",
    "\n",
    "# f = open(\"SVCTake1.txt\", \"w+\")\n",
    "# f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrainBal, yTrainBal = sm.fit_sample(xTrainNT, yTrainDist.ravel())\n",
    "\n",
    "# scale = StandardScaler().fit(xTrainBal)\n",
    "# xTrainScaled = scale.transform(xTrainBal)\n",
    "# xTestScaled = scale.transform(xTestNT)\n",
    "\n",
    "# svc = svm.SVC(kernel=\"poly\", probability=True)\n",
    "# svc.fit(xTrainScaled, yTrainBal)\n",
    "# probs = svc.predict_proba(xTestScaled)\n",
    "# ids = xTestIds.tolist()\n",
    "# probs = probs.tolist()\n",
    "\n",
    "# f = open(\"SVCTake2.txt\", \"w+\")\n",
    "# f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrainBal, yTrainBal = sm.fit_sample(xTrainNT, yTrainDist.ravel())\n",
    "\n",
    "# scale = StandardScaler().fit(xTrainBal)\n",
    "# xTrainScaled = scale.transform(xTrainBal)\n",
    "# xTestScaled = scale.transform(xTestNT)\n",
    "\n",
    "# svc = svm.SVC(kernel=\"linear\", probability=True)\n",
    "# svc.fit(xTrainScaled, yTrainBal)\n",
    "# probs = svc.predict_proba(xTestScaled)\n",
    "# ids = xTestIds.tolist()\n",
    "# probs = probs.tolist()\n",
    "\n",
    "# f = open(\"SVCTake2Extra.txt\", \"w+\")\n",
    "# f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrainBal, yTrainBal = sm.fit_sample(xTrainImp, yTrainDist.ravel())\n",
    "# # Countdown for hidden layers and perceptrons\n",
    "# scale = StandardScaler().fit(xTrainBal)\n",
    "# xTrainScaled = scale.transform(xTrainBal)\n",
    "# xTestScaled = scale.transform(xTestImp)\n",
    "\n",
    "# clf = MLPClassifier(activation='relu',hidden_layer_sizes=(10,9,8,7,6,5,4,3,2,1), random_state=1, max_iter=2000)\n",
    "# clf.fit(xTrainScaled, yTrainBal)\n",
    "# probs = clf.predict_proba(xTestScaled)\n",
    "# ids = xTestIds.tolist()\n",
    "# probs = probs.tolist()\n",
    "\n",
    "# f = open(\"NeuralNetworkTake1.txt\", \"w+\")\n",
    "# f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "# f.close()\n",
    "# print(\"Converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# xTrainBal, yTrainBal = sm.fit_sample(xTrainNT, yTrainDist.ravel())\n",
    "# # This is for...\n",
    "# scale = StandardScaler().fit(xTrainBal)\n",
    "# xTrainScaled = scale.transform(xTrainBal)\n",
    "# xTestScaled = scale.transform(xTestNT)\n",
    "\n",
    "# clf = MLPClassifier(activation='tanh',hidden_layer_sizes=(10,9,8,7,6,5,4,3,2,1), random_state=1, max_iter=2000)\n",
    "# clf.fit(xTrainScaled, yTrainBal)\n",
    "# probs = clf.predict_proba(xTestScaled)\n",
    "# ids = xTestIds.tolist()\n",
    "# probs = probs.tolist()\n",
    "\n",
    "# f = open(\"NeuralNetworkTake2.txt\", \"w+\")\n",
    "# f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "# f.close()\n",
    "# print(\"Converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# xTrainBal, yTrainBal = sm.fit_sample(xTrainImp, yTrainDist.ravel())\n",
    "# # This is for...\n",
    "# scale = StandardScaler().fit(xTrainBal)\n",
    "# xTrainScaled = scale.transform(xTrainBal)\n",
    "# xTestScaled = scale.transform(xTestImp)\n",
    "\n",
    "# clf = MLPClassifier(activation='tanh',hidden_layer_sizes=(2), random_state=1, max_iter=2000)\n",
    "# clf.fit(xTrainScaled, yTrainBal)\n",
    "# probs = clf.predict_proba(xTestScaled)\n",
    "# ids = xTestIds.tolist()\n",
    "# probs = probs.tolist()\n",
    "\n",
    "# f = open(\"NeuralNetworkTake3.txt\", \"w+\")\n",
    "# f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "# f.close()\n",
    "# print(\"Converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# xTrainBal, yTrainBal = sm.fit_sample(xTrainImp, yTrainDist.ravel())\n",
    "# # This is for...\n",
    "# scale = StandardScaler().fit(xTrainBal)\n",
    "# xTrainScaled = scale.transform(xTrainBal)\n",
    "# xTestScaled = scale.transform(xTestImp)\n",
    "\n",
    "# clf = MLPClassifier(activation='logistic',hidden_layer_sizes=(2), random_state=1, max_iter=2000, learning_rate_init=11)\n",
    "# clf.fit(xTrainScaled, yTrainBal)\n",
    "# probs = clf.predict_proba(xTestScaled)\n",
    "# ids = xTestIds.tolist()\n",
    "# probs = probs.tolist()\n",
    "\n",
    "# f = open(\"NeuralNetworkTake4.txt\", \"w+\")\n",
    "# f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "# f.close()\n",
    "# print(\"Converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged\n"
     ]
    }
   ],
   "source": [
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainImp, yTrainDist.ravel())\n",
    "# This is for...\n",
    "scale = StandardScaler().fit(xTrainBal)\n",
    "xTrainScaled = scale.transform(xTrainBal)\n",
    "xTestScaled = scale.transform(xTestImp)\n",
    "\n",
    "clf = MLPClassifier(activation='logistic',hidden_layer_sizes=(15), random_state=1, max_iter=2000, learning_rate_init=10)\n",
    "clf.fit(xTrainScaled, yTrainBal)\n",
    "probs = clf.predict_proba(xTestScaled)\n",
    "ids = xTestIds.tolist()\n",
    "probs = probs.tolist()\n",
    "\n",
    "f = open(\"NeuralNetworkTake5.txt\", \"w+\")\n",
    "f.write(\"Unique Id,DIST\\n\")\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    f.write(str(int(ids[i])) + \",\" + str(probs[i][1]) + \"\\n\")\n",
    "    \n",
    "f.close()\n",
    "print(\"Converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
