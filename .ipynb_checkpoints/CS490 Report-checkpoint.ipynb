{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS490 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifiers                | Score on Training | Score on Validation | Score on Leaderboard |\n",
    "|----------------------------|-------------------|---------------------|----------------------|\n",
    "| KNN (20)                   | 94.8%             | 89.8%               | 62.8%                |\n",
    "| SVM (sigmoid)              | 82.04%            | 84.6%               | 82.6%                |\n",
    "| Neural Network(15 neurons) | 72.4%             | 91.8%               | 81.2%                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code used for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm #Import classifier\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTestData = np.genfromtxt(\"data/X_test.txt\", delimiter = None, skip_header=1)\n",
    "xTrainData = np.genfromtxt(\"data/X_train.txt\", delimiter = None, skip_header=1)\n",
    "yTrainData = np.genfromtxt(\"data/Y_train.txt\", delimiter = None, skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrainData, yTrainData, test_size=0.25, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainDist = yTrain[:, 1]\n",
    "yValDist = yVal[:,1]\n",
    "\n",
    "#OverSampler\n",
    "sm = SMOTE(random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first model we decided to use a KNN Classifier to try and predict the data. Our best KNN classifier used the features: organization size, audit firm size, organization form, current ratio, operating margin, equity financing ratio, current asset turnover, debt ratio, and total asset turnover. The way we picked these features was through a lot of trial and error, but we also made many graphs to try and examine the relationships between different features. An important parameter for our classifier was to determine our k-value. This was not difficult to pick for us because when we performed our score methods, it was obvious that we wanted to use something in the middle of between five and fifty. The reason was because five was scoring too high (98.4%) and fifty was scoring to low (70.2%), and thus we came up with using the sweet spot of twenty as our k-value (89.8%). With these ideas in our head, when it was scored, we managed to get a 62.8% on the final validation data on the Kaggle leaderboard.\n",
    "\n",
    "The most fun from working with this project was how we implemented the KNN classifier. It was implemented by using the documentation on sci-kit learn. Sci-kit learn had a simple to use KNN classifier, and it gave us all the information on how to create the model, how to score the model, and how to adjust parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the models that we trained were tested on multiple sets of features. For this model, we did reserach to find if we can use some code to figure out what the important features are for this dataset. We found that we can use the ExtraTrees classifier to see the important features. We trained that classifer with all of the features from the training data text file. This returned the results that size, current ratio, quick ratio, total margin, return on equity, net working capital, return on assets, long-term debt to equity, equity financing ratio, and debt ratio are the 10 most important features. This can be seen in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "xTrainData = np.genfromtxt(\"data/X_train.txt\", delimiter = None, skip_header=1)\n",
    "yTrainData = np.genfromtxt(\"data/Y_train.txt\", delimiter = None, skip_header=1)\n",
    "X = xTrainData[:,1:35]  #independent columns\n",
    "y = yTrainData[:,1]    #target column i.e price range\n",
    "\n",
    "scale = StandardScaler().fit(X)\n",
    "xTrainScaled = scale.transform(X)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(xTrainScaled,y)\n",
    "# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data from the text file was split into 75% training data and 25% validation data. The training data was then balanced using oversampling. The training and test data were also scaled. The SVC model was trained on the 75% split. The SVC also had different options for the kernel function. Sci-kit learn gave the options to have a linear, poly, rbf and sigmoid. The one that worked the best for us was the sigmoid kernel function. To figure out which kernel function worked the best we looked at the confusion matrix and the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network was the most challenging for Nick and I since at first we thought having a lot of hidden layers and lot of neurons at each layer was how these models should be trained. We later realized that more hidden layers and neurons was on the track more towards deep learning. For this, we will talk about our best neural network performer.\n",
    "\n",
    "Our best performer was trained on one feature, the total margin. The training data was also balanced using oversampling. We did plenty of runs with all of their different activation functions, max iterations, and the step sizes for gradient descent. We kept the default value for gradient descent since the others seemed to overfit or underfit our data each time.  We kept a max iteration of 2000, since we were getting warnings about our models not converging with a lower value.  The activation function options were identity, logistic, tanh, and relu. Logistic worked the best for the total margin feature. We chose to use one hidden layer with 15 neurons. Our scoring function ran scores for one hidden layer and tried 1-25 neurons. After multiple runs, 15 neurons seemed to appear the most with the best score and best-looking confusion matrix (This is explained later). A higher step size for gradient descent also performed best for this model. We chose a step size of 10. Anything more or anything less for this number of hidden layers did not perform well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Performer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best performer was the Support Vector classifier with the sigmoid kernel function. For our best prediction we used 10 features from the data. A Support vector Machine worked well for this since they tend to be effective for high dimensional spaces. With sci-kit learn the prediction probability function also ran the training through cross-validation making the model more accurate and precise. For Support Vector Machines, it maximizes the margin between the two classes, in this case its whether the hospital is in financial distress or not. On top of that, we trained the data with a sigmoid kernel function which would help the model train more effectively by adding curves to the decision boundries. The margins between the support vectors were not just linear. We could not graph 10 features together, but the data most likely was shaped in a way where finding the maximal margin for the decision boundry separated the data enough to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainImp = xTrain[:,[1,4,5,11,13,8,14,22,20,27]]\n",
    "xValImp = xVal[:,[1,4,5,11,13,8,14,22,20,27]]\n",
    "xTestImp = xTestData[:,[1,4,5,11,13,8,14,22,20,27]]\n",
    "xTrainBal, yTrainBal = sm.fit_sample(xTrainImp, yTrainDist.ravel())\n",
    "\n",
    "scale = StandardScaler().fit(xTrainBal)\n",
    "xTrainScaled = scale.transform(xTrainBal)\n",
    "xTestScaled = scale.transform(xTestImp)\n",
    "\n",
    "svc = svm.SVC(kernel=\"sigmoid\", probability=True)\n",
    "svc.fit(xTrainScaled, yTrainBal)\n",
    "probs = svc.predict_proba(xTestScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Good, The Bad, and Honorable Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of good performers neural networks with one or two hidden layers with a logistic activation function and between 10-15 neurons at each layer performed particularly well. We did three submissions using those parameters. We trained the models with total margin as the only feature, which was balanced using oversampling. One of the scores was 74% and the other two were 81%. For this data, moving in the direction of deep learning is ineffective. More hidden layers caused overfitting since the data was not complex enough.\n",
    "\n",
    "The performer that was not so great was K-Nearest neighbors. The best leaderboard score was 62.8%. There is one problem when it comes to working with a nearest neighbor classifier, the curse of dimensionality. The curse of dimensionality says that in higher dimensions, almost all data are equally far away. This becomes a huge problem because if we are classifying based upon the closest points, then our model is not going to understand which data is closer. Thus, the accuracy of our model is going to decrease substantially. This is the reason why we wanted to limit ourselves to the number of features we used, 10/35. Of course, we wanted to see if we could get something higher than a 62.8%, but we never could. We made the decision that KNN might just not be the approach to classifying this data because it was too complex for the way KNN models train the data.\n",
    "\n",
    "There are a few more honorable mentions when looking at the the performance of all of the models. Our highest scores on the leaderboard had the same trend when it came to the values of the confusion matrix. When the score function returned a score of around 90% on the validation set, and the confusion matrix favored false negatives, we tended to score 75% or higher on the leaderboard. On top of that, for these same results, the f-1 score for not in distress was around 91 percent and the f-1 score for in distress was around 40%. The models that returned less false negatives did poorly on the test data. Trying to maximaize the f-1 scores also showed poor results on the test data. In all, favoring false negatives seemed to play a key role in performance on the test data. Having a higher number of false negatives proved to be a good way of balancing underfitting and overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
